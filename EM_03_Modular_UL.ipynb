{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EM_03_Modular_UL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOKbtMCX3ZWO1E/aTtVsmd0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssvadla/Legal_Text_Classification/blob/main/EM_03_Modular_UL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJaX0B4lWLL7",
        "outputId": "5dd43133-98e7-4e1a-91dd-8f82514088be"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "import math\n",
        "from sklearn.utils import resample\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Read the labeled data\n",
        "train1 = pd.read_csv('/content/drive/My Drive/Research/train_data1.csv')\n",
        "train2 = pd.read_csv('/content/drive/My Drive/Research/train_data2.csv')\n",
        "train3 = pd.read_csv('/content/drive/My Drive/Research/train_data3.csv')\n",
        "train4 = pd.read_csv('/content/drive/My Drive/Research/train_data4.csv')\n",
        "train5 = pd.read_csv('/content/drive/My Drive/Research/train_data5.csv')\n",
        "train6 = pd.read_csv('/content/drive/My Drive/Research/train_data6.csv')\n",
        "train7 = pd.read_csv('/content/drive/My Drive/Research/train_data7.csv')\n",
        "train8 = pd.read_csv('/content/drive/My Drive/Research/train_data8.csv')\n",
        "train9 = pd.read_csv('/content/drive/My Drive/Research/train_data9.csv')\n",
        "train10 = pd.read_csv('/content/drive/My Drive/Research/train_data10.csv')\n",
        "train_highKappa = pd.read_csv('/content/drive/My Drive/Research/train_data_highkappa.csv')\n",
        "train = train1\n",
        "train_list = [train2,train3,train4,train5,train6,train7,train8,train9,train10,train_highKappa]\n",
        "for i in train_list:\n",
        "  train = train.append(i)\n",
        "train.sort_values(\"Sentence\", inplace = True)\n",
        "new_train = train.drop_duplicates(subset =\"Sentence\")\n",
        "train = new_train\n",
        "train['Target'].unique()\n",
        "train['Target']=train['Target'].replace(['Others'],'Invalid')\n",
        "train['Target'].unique()\n",
        "\n",
        "#cleaning\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stopword=nltk.corpus.stopwords.words('english')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wl= WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "  text=\"\".join([word.lower() for word in text if word not in string.punctuation])\n",
        "  tokens = re.split('\\W+',text)\n",
        "  text = [wl.lemmatize(word) for word in tokens if word not in stopword]\n",
        "  return text\n",
        "len(train['Sentence'])\n",
        "text = clean_text(train['Sentence'])\n",
        "tfidf_vect = TfidfVectorizer(analyzer = clean_text)\n",
        "X_tfidf = tfidf_vect.fit_transform(train['Sentence'])\n",
        "print(X_tfidf.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "37711\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "(4416, 7374)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33rsHagDz1wm"
      },
      "source": [
        "#Read the test data\n",
        "test = pd.read_csv(r'/content/drive/My Drive/Research/test_data.csv')\n",
        "test['Target']=test['Target'].replace(['Others'],'Invalid')\n",
        "test['Sentence'] = test['Sentence'].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\n",
        "test['Sentence'] = test['Sentence'].str.replace('[^\\w\\s]','')\n",
        "from nltk.corpus import stopwords\n",
        "words = stopwords.words('english')\n",
        "test['Sentence'] = test['Sentence'].apply(lambda x: \" \".join(x for x in x.split() if x not in words))\n",
        "t_p = tfidf_vect.transform(test['Sentence'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ny7SbxbIz-cM",
        "outputId": "76530478-fd3f-4271-e954-6b7f7a451ddf"
      },
      "source": [
        "#Read the unlabeled data\n",
        "unlabel = pd.read_csv(r'/content/drive/My Drive/Research/Unlabeled_data.csv')\n",
        "del unlabel['Complete']\n",
        "del unlabel['Unnamed: 0']\n",
        "\n",
        "unlabel['text'] = unlabel['text'].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\n",
        "unlabel['text'] = unlabel['text'].str.replace('[^\\w\\s]','')\n",
        "from nltk.corpus import stopwords\n",
        "words = stopwords.words('english')\n",
        "unlabel['text'] = unlabel['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in words))\n",
        "\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "unlabel['text'] = unlabel['text'].apply(lambda x: TextBlob(x).words)\n",
        "unlabel['text'] = unlabel['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x]))\n",
        "\n",
        "def index_reset(unlabel_2):\n",
        "  unlabel_2.reset_index(inplace=True)\n",
        "  del unlabel_2['index']\n",
        "  return unlabel_2\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bakKN8gF1cdi"
      },
      "source": [
        "def feature_set_selection(train_and_unlabel_words_wo_duplicates,train_words,unlabel_words):\n",
        "  for term in train_and_unlabel_words_wo_duplicates:\n",
        "    freq_train = train_words.count(term) / len(train_words)\n",
        "    freq_unlabel = unlabel_words.count(term) / len(unlabel_words)\n",
        "    if freq_unlabel == 0:\n",
        "      PF.append(term)\n",
        "    elif (freq_train // freq_unlabel) > Threshold_feature:\n",
        "      PF.append(term)\n",
        "    else:\n",
        "      NF.append(term)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSwX70qa1lgk"
      },
      "source": [
        "\n",
        "#Identifying  a  set  of  reliable  negative  documents from the unlabeled set\n",
        "def RN_selection(RN, unlabel_1):\n",
        "  iteration_RN = 0\n",
        "  RN_to_be_removed = []\n",
        "  pos_to_be_removed = []\n",
        "  freq_each_word_list = []\n",
        "  Q_pos = []\n",
        "  count = 0\n",
        "  if_count = 0\n",
        "  for doc in unlabel_1['text']:\n",
        "    doc_words = clean_text(doc)\n",
        "    doc_words_wo_duplicates = list(set(doc_words))\n",
        "    for_count = 0\n",
        "    \n",
        "    for each_doc_word in doc_words_wo_duplicates:\n",
        "      for_count = for_count + 1\n",
        "      freq_each_doc_word = doc_words.count(each_doc_word)\n",
        "      freq_each_word_list.append(freq_each_doc_word)\n",
        "      if (freq_each_doc_word > 0) and (each_doc_word in PF):\n",
        "        if_count = if_count + 1\n",
        "        pos_to_be_removed.append(count)\n",
        "        RN_to_be_removed.append(doc)\n",
        "        break\n",
        "      \n",
        "    if for_count == len(doc_words_wo_duplicates):\n",
        "      Q_pos.append(count)\n",
        "    count = count + 1\n",
        "    iteration_RN = iteration_RN + 1\n",
        "  RN.drop(pos_to_be_removed,axis=0,inplace=True)\n",
        "  return RN,pos_to_be_removed, Q_pos, if_count\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDmtdb7a12e3"
      },
      "source": [
        "\n",
        "def classifier_select(train, Q, RN):\n",
        "  f1_score_list = []\n",
        "  per_CR_list = []\n",
        "  loop_variable =0\n",
        "  while(1):\n",
        "    p_and_RN = pd.concat([train,RN])\n",
        "    p_and_RN.reset_index(inplace=True,drop=True)\n",
        "    p_and_RN_vect = tfidf_vect.transform(p_and_RN['text'])\n",
        "    p_and_RN_vect_df=pd.DataFrame(p_and_RN_vect.toarray())\n",
        "\n",
        "    Q_vect = tfidf_vect.transform(Q['text'])\n",
        "    Q_vect_df=pd.DataFrame(Q_vect.toarray())\n",
        "\n",
        "    lgb_classifier = lgb.LGBMClassifier()\n",
        "    lgb_classifier.fit(p_and_RN_vect_df, p_and_RN['Target'])\n",
        "    np.unique(p_and_RN['Target'])\n",
        "\n",
        "    #checking the classifier if it gives best results\n",
        "    train_vect = tfidf_vect.transform(train['text'])\n",
        "    train_vect_df=pd.DataFrame(train_vect.toarray())\n",
        "    train_pred = lgb_classifier.predict(train_vect_df)\n",
        "    classified_negative = (train_pred.tolist()).count(-1)\n",
        "    percentage = (classified_negative / len(train_pred)) * 100\n",
        "    model_list.append(lgb_classifier)\n",
        "    if percentage < percent_thresh:\n",
        "      thresh_model_list.append(lgb_classifier)\n",
        "      test_pred = lgb_classifier.predict(t_p.toarray())\n",
        "      test['Target']= LabelEncoder().fit_transform(test['Target'])\n",
        "      classification_report_test = classification_report(test['Target'],test_pred,digits=4)\n",
        "      f1_score_test = f1_score(test['Target'],test_pred,average='weighted')\n",
        "      f1_score_list.append(f1_score_test)\n",
        "      per_CR_list.append(classification_report_test)\n",
        "    Q_pred = lgb_classifier.predict(Q_vect_df)\n",
        "    np.unique(Q_pred)\n",
        "    count_q = 0\n",
        "    total_q = 0 \n",
        "    out_pos_q = []\n",
        "    pos_q = []\n",
        "    for i in Q_pred:\n",
        "      if i == -1:\n",
        "        pos_q.append(count_q)\n",
        "        total_q = total_q + 1\n",
        "      else:\n",
        "        out_pos_q.append(count_q)\n",
        "      count_q = count_q + 1\n",
        "    Q.reset_index(inplace=True,drop=True)\n",
        "    W = Q.loc[pos_q,:]\n",
        "    if W.empty :\n",
        "      print(\"W is empty, came out of loop\")\n",
        "      break\n",
        "    else:\n",
        "      Q_new = Q.loc[out_pos_q,:]\n",
        "      Q = Q_new.copy(deep =True)\n",
        "      RN = pd.concat([RN,W])\n",
        "      RN.reset_index(inplace=True,drop=True)\n",
        "      loop_variable = loop_variable + 1\n",
        "    print(\"completed iteration\")\n",
        "  result_dict['F1_score'].append(f1_score_list)\n",
        "  result_dict['CR'].append(per_CR_list)\n",
        "  return lgb_classifier, model_list , thresh_model_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4Wg303W17cP"
      },
      "source": [
        "model_list = []\n",
        "thresh_model_list = []\n",
        "UL_size_list = []\n",
        "result_dict = {\"F1_score\":[],\"CR\":[],\"Unlabel_size\":[]}\n",
        "Threshold_feature = 1\n",
        "percent_thresh = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lHnJPpzRyIg"
      },
      "source": [
        "unlabel_range = [5000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WgzChbk2sIi",
        "outputId": "fd6f7d9a-865f-4560-b4e8-2c5e95bb5035"
      },
      "source": [
        "\n",
        "for size in unlabel_range:\n",
        "  unlabel_1 = unlabel.loc[:size]\n",
        "  unlabel_1 = index_reset(unlabel_1)\n",
        "  unlabel_1_copy = unlabel_1.copy(deep = True)\n",
        "  x_un1 = tfidf_vect.transform(unlabel_1['text'])\n",
        "  PF = []\n",
        "  NF = []\n",
        "  unlabel_1['Target']=-1\n",
        "  train = train.rename(columns={'Sentence':'text'})\n",
        "  train['Target']= LabelEncoder().fit_transform(train['Target'])\n",
        "  train_and_unlabel =  pd.concat([train,unlabel_1])\n",
        "  print(len(train))\n",
        "  print(len(unlabel_1))\n",
        "  print(len(train_and_unlabel))\n",
        "  train = train.rename(columns={'Sentence':'text'})\n",
        "  #mixed sampling\n",
        "  train_and_unlabel =  pd.concat([train,unlabel_1])\n",
        "  print(\"len of train_and_unlabel\",len(train_and_unlabel))\n",
        "  train_and_unlabel.reset_index(inplace=True)\n",
        "  del train_and_unlabel['index']\n",
        "  del train_and_unlabel['Unnamed: 0']\n",
        "  target_values = np.unique(train_and_unlabel['Target'].values)\n",
        "  data_list = []\n",
        "  data_length_list = []\n",
        "  for i in target_values:\n",
        "    df_k = train_and_unlabel[train_and_unlabel['Target']==i]\n",
        "    data_list.append(df_k)\n",
        "    data_length_list.append(len(df_k))\n",
        "  maximum_data = max(data_length_list)\n",
        "  ratio = math.floor(( 4 / 6 )* maximum_data)\n",
        "  loop_count = 0\n",
        "  train_upsampled = []\n",
        "  for i in data_length_list:\n",
        "    if i < ratio:\n",
        "      df_upsampled = resample(data_list[loop_count],replace=True,n_samples=ratio,random_state=123)\n",
        "    else:\n",
        "      df_upsampled = resample(data_list[loop_count],replace=True,n_samples=i,random_state=123)\n",
        "    train_upsampled.append(df_upsampled)\n",
        "    loop_count = loop_count + 1\n",
        "\n",
        "  df_res = pd.concat(train_upsampled)\n",
        "  train_and_unlabel = df_res.copy(deep = True)\n",
        "  print(\"len of train_and_unlabel\",len(train_and_unlabel))\n",
        "  train['Target']= LabelEncoder().fit_transform(train['Target'])\n",
        "  train_and_unlabel_words = clean_text(train_and_unlabel['text'])\n",
        "  train_and_unlabel_words_wo_duplicates = list(set(train_and_unlabel_words))\n",
        "  train_words = clean_text(train['text'])\n",
        "  train_words_wo_duplicates = list(set(train_words))\n",
        "  unlabel_words = clean_text(unlabel_1['text'])\n",
        "  unlabel_words_wo_duplicates = list(set(unlabel_words))\n",
        "  feature_set_selection(train_and_unlabel_words_wo_duplicates,train_words,unlabel_words)\n",
        "  RN = unlabel_1.copy(deep=True) \n",
        "  RN,op_pos_to_be_removed, op_Q_pos, op_if_count = RN_selection(RN,unlabel_1) \n",
        "  Q =  unlabel_1.loc[op_pos_to_be_removed,:]\n",
        "  result_dict['Unlabel_size'].append(size) \n",
        "  lgb_classifier, op_model_list, op_thresh_model_list = classifier_select(train, Q, RN)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Unnamed: 0', 'Sentence', 'Target'], dtype='object')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4416\n",
            "5001\n",
            "9417\n",
            "len of train_and_unlabel 9417\n",
            "maximum_data 5001\n",
            "ratio 3334\n",
            "@ 5001\n",
            "@ 3334\n",
            "@ 3334\n",
            "@ 3334\n",
            "@ 3334\n",
            "@ 3334\n",
            "@ 3334\n",
            "len of train_and_unlabel 25005\n",
            "5001\n",
            "0\n",
            "#######target unique [-1  0  1  2  3  4  5]\n",
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 675\n",
            "len(train predic 4416\n",
            "completed iteration\n",
            "1\n",
            "#######target unique [-1  0  1  2  3  4  5]\n",
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 882\n",
            "len(train predic 4416\n",
            "completed iteration\n",
            "2\n",
            "#######target unique [-1  0  1  2  3  4  5]\n",
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 948\n",
            "len(train predic 4416\n",
            "completed iteration\n",
            "3\n",
            "#######target unique [-1  0  1  2  3  4  5]\n",
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 971\n",
            "len(train predic 4416\n",
            "completed iteration\n",
            "4\n",
            "#######target unique [-1  0  1  2  3  4  5]\n",
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 990\n",
            "len(train predic 4416\n",
            "completed iteration\n",
            "5\n",
            "#######target unique [-1  0  1  2  3  4  5]\n",
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 1007\n",
            "len(train predic 4416\n",
            "completed iteration\n",
            "6\n",
            "#######target unique [-1  0  1  2  3  4  5]\n",
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 1004\n",
            "len(train predic 4416\n",
            "completed iteration\n",
            "7\n",
            "#######target unique [-1  0  1  2  3  4  5]\n",
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 989\n",
            "len(train predic 4416\n",
            "completed iteration\n",
            "8\n",
            "#######target unique [-1  0  1  2  3  4  5]\n",
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 1007\n",
            "len(train predic 4416\n",
            "completed iteration\n",
            "9\n",
            "#######target unique [-1  0  1  2  3  4  5]\n",
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 1001\n",
            "len(train predic 4416\n",
            "completed iteration\n",
            "10\n",
            "#######target unique [-1  0  1  2  3  4  5]\n",
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 1023\n",
            "len(train predic 4416\n",
            "completed iteration\n",
            "11\n",
            "#######target unique [-1  0  1  2  3  4  5]\n",
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 1020\n",
            "len(train predic 4416\n",
            "completed iteration\n",
            "12\n",
            "#######target unique [-1  0  1  2  3  4  5]\n",
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 1013\n",
            "len(train predic 4416\n",
            "completed iteration\n",
            "13\n",
            "#######target unique [-1  0  1  2  3  4  5]\n",
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 1020\n",
            "len(train predic 4416\n",
            "completed iteration\n",
            "14\n",
            "#######target unique [-1  0  1  2  3  4  5]\n",
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 1016\n",
            "len(train predic 4416\n",
            "completed iteration\n",
            "15\n",
            "#######target unique [-1  0  1  2  3  4  5]\n",
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 1021\n",
            "len(train predic 4416\n",
            "completed iteration\n",
            "16\n",
            "#######target unique [-1  0  1  2  3  4  5]\n",
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 1024\n",
            "len(train predic 4416\n",
            "completed iteration\n",
            "17\n",
            "#######target unique [-1  0  1  2  3  4  5]\n",
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 1032\n",
            "len(train predic 4416\n",
            "completed iteration\n",
            "18\n",
            "#######target unique [-1  0  1  2  3  4  5]\n",
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 1018\n",
            "len(train predic 4416\n",
            "completed iteration\n",
            "19\n",
            "#######target unique [-1  0  1  2  3  4  5]\n",
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 1037\n",
            "len(train predic 4416\n",
            "completed iteration\n",
            "20\n",
            "#######target unique [-1  0  1  2  3  4  5]\n",
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 1017\n",
            "len(train predic 4416\n",
            "completed iteration\n",
            "21\n",
            "#######target unique [-1  0  1  2  3  4  5]\n",
            "#############train pred [-1  0  1  2  3  4  5]\n",
            "unique predicted [-1  0  1  2  3  4  5]\n",
            "classified_negative 1027\n",
            "len(train predic 4416\n",
            "W is empty, came out of loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-o0CDWp3GtNC"
      },
      "source": [
        "with open(r'/content/drive/My Drive/Results/EM/Result01.txt', 'a') as writefile:\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(\" EM Results \")\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(\" final dict \")\n",
        "        writefile.write(\"\\n\")\n",
        "        writefile.write(str(result_dict))\n",
        "        writefile.write(\"\\n\")\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8kRpMq82Cy7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50d98789-3d51-4e74-a7e7-8bde08d70dcf"
      },
      "source": [
        "\n",
        "for model in op_model_list:\n",
        "  test_pred = model.predict(t_p.toarray())\n",
        "  test['Target']= LabelEncoder().fit_transform(test['Target'])\n",
        "  classification_report_test = classification_report(test['Target'],test_pred,digits=4)\n",
        "  print(classification_report_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4615    0.1558    0.2330        77\n",
            "           1     0.7619    0.6154    0.6809        26\n",
            "           2     0.7467    0.6292    0.6829       267\n",
            "           3     0.6944    0.3012    0.4202        83\n",
            "           4     0.9091    0.2941    0.4444        34\n",
            "           5     0.6190    0.3824    0.4727        34\n",
            "\n",
            "    accuracy                         0.4683       521\n",
            "   macro avg     0.5990    0.3397    0.4192       521\n",
            "weighted avg     0.6992    0.4683    0.5452       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.5000    0.1299    0.2062        77\n",
            "           1     0.7500    0.5769    0.6522        26\n",
            "           2     0.7766    0.5730    0.6595       267\n",
            "           3     0.6897    0.2410    0.3571        83\n",
            "           4     0.8182    0.2647    0.4000        34\n",
            "           5     0.6000    0.3529    0.4444        34\n",
            "\n",
            "    accuracy                         0.4203       521\n",
            "   macro avg     0.5906    0.3055    0.3885       521\n",
            "weighted avg     0.7118    0.4203    0.5130       521\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4545    0.1299    0.2020        77\n",
            "           1     0.7895    0.5769    0.6667        26\n",
            "           2     0.7672    0.5431    0.6360       267\n",
            "           3     0.6667    0.2410    0.3540        83\n",
            "           4     0.8889    0.2353    0.3721        34\n",
            "           5     0.5000    0.2941    0.3704        34\n",
            "\n",
            "    accuracy                         0.3992       521\n",
            "   macro avg     0.5810    0.2886    0.3716       521\n",
            "weighted avg     0.6966    0.3992    0.4939       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4762    0.1299    0.2041        77\n",
            "           1     0.7895    0.5769    0.6667        26\n",
            "           2     0.7654    0.5131    0.6143       267\n",
            "           3     0.6800    0.2048    0.3148        83\n",
            "           4     0.9000    0.2647    0.4091        34\n",
            "           5     0.5455    0.3529    0.4286        34\n",
            "\n",
            "    accuracy                         0.3839       521\n",
            "   macro avg     0.5938    0.2918    0.3768       521\n",
            "weighted avg     0.7047    0.3839    0.4831       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4545    0.1299    0.2020        77\n",
            "           1     0.7500    0.5769    0.6522        26\n",
            "           2     0.7753    0.5169    0.6202       267\n",
            "           3     0.6923    0.2169    0.3303        83\n",
            "           4     0.8889    0.2353    0.3721        34\n",
            "           5     0.5455    0.3529    0.4286        34\n",
            "\n",
            "    accuracy                         0.3858       521\n",
            "   macro avg     0.5866    0.2898    0.3722       521\n",
            "weighted avg     0.7058    0.3858    0.4851       521\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4762    0.1299    0.2041        77\n",
            "           1     0.7500    0.5769    0.6522        26\n",
            "           2     0.7824    0.4981    0.6087       267\n",
            "           3     0.7826    0.2169    0.3396        83\n",
            "           4     0.7500    0.2647    0.3913        34\n",
            "           5     0.5000    0.2941    0.3704        34\n",
            "\n",
            "    accuracy                         0.3743       521\n",
            "   macro avg     0.5773    0.2829    0.3666       521\n",
            "weighted avg     0.7150    0.3743    0.4785       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4762    0.1299    0.2041        77\n",
            "           1     0.7895    0.5769    0.6667        26\n",
            "           2     0.7706    0.4906    0.5995       267\n",
            "           3     0.6957    0.1928    0.3019        83\n",
            "           4     0.7273    0.2353    0.3556        34\n",
            "           5     0.5000    0.2941    0.3704        34\n",
            "\n",
            "    accuracy                         0.3647       521\n",
            "   macro avg     0.5656    0.2742    0.3569       521\n",
            "weighted avg     0.6956    0.3647    0.4661       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4737    0.1169    0.1875        77\n",
            "           1     0.7895    0.5769    0.6667        26\n",
            "           2     0.7719    0.4944    0.6027       267\n",
            "           3     0.6957    0.1928    0.3019        83\n",
            "           4     0.7500    0.2647    0.3913        34\n",
            "           5     0.5238    0.3235    0.4000        34\n",
            "\n",
            "    accuracy                         0.3685       521\n",
            "   macro avg     0.5721    0.2813    0.3643       521\n",
            "weighted avg     0.6990    0.3685    0.4696       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4545    0.1299    0.2020        77\n",
            "           1     0.7500    0.5769    0.6522        26\n",
            "           2     0.7738    0.4869    0.5977       267\n",
            "           3     0.7273    0.1928    0.3048        83\n",
            "           4     0.9000    0.2647    0.4091        34\n",
            "           5     0.5455    0.3529    0.4286        34\n",
            "\n",
            "    accuracy                         0.3685       521\n",
            "   macro avg     0.5930    0.2863    0.3706       521\n",
            "weighted avg     0.7114    0.3685    0.4719       521\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.5000    0.1299    0.2062        77\n",
            "           1     0.7895    0.5769    0.6667        26\n",
            "           2     0.7870    0.4981    0.6101       267\n",
            "           3     0.6957    0.1928    0.3019        83\n",
            "           4     0.8889    0.2353    0.3721        34\n",
            "           5     0.5455    0.3529    0.4286        34\n",
            "\n",
            "    accuracy                         0.3724       521\n",
            "   macro avg     0.6009    0.2837    0.3694       521\n",
            "weighted avg     0.7210    0.3724    0.4767       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.5000    0.1429    0.2222        77\n",
            "           1     0.7895    0.5769    0.6667        26\n",
            "           2     0.7733    0.4981    0.6059       267\n",
            "           3     0.6957    0.1928    0.3019        83\n",
            "           4     0.7778    0.2059    0.3256        34\n",
            "           5     0.5263    0.2941    0.3774        34\n",
            "\n",
            "    accuracy                         0.3685       521\n",
            "   macro avg     0.5804    0.2730    0.3571       521\n",
            "weighted avg     0.7055    0.3685    0.4706       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.5500    0.1429    0.2268        77\n",
            "           1     0.7895    0.5769    0.6667        26\n",
            "           2     0.7738    0.4869    0.5977       267\n",
            "           3     0.6957    0.1928    0.3019        83\n",
            "           4     0.8889    0.2353    0.3721        34\n",
            "           5     0.5000    0.3235    0.3929        34\n",
            "\n",
            "    accuracy                         0.3666       521\n",
            "   macro avg     0.5997    0.2798    0.3654       521\n",
            "weighted avg     0.7187    0.3666    0.4711       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4286    0.1169    0.1837        77\n",
            "           1     0.7895    0.5769    0.6667        26\n",
            "           2     0.7661    0.4906    0.5982       267\n",
            "           3     0.6957    0.1928    0.3019        83\n",
            "           4     0.7778    0.2059    0.3256        34\n",
            "           5     0.5000    0.2647    0.3462        34\n",
            "\n",
            "    accuracy                         0.3589       521\n",
            "   macro avg     0.5654    0.2640    0.3460       521\n",
            "weighted avg     0.6895    0.3589    0.4589       521\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4737    0.1169    0.1875        77\n",
            "           1     0.7895    0.5769    0.6667        26\n",
            "           2     0.7574    0.4794    0.5872       267\n",
            "           3     0.7619    0.1928    0.3077        83\n",
            "           4     0.8000    0.2353    0.3636        34\n",
            "           5     0.5000    0.2941    0.3704        34\n",
            "\n",
            "    accuracy                         0.3570       521\n",
            "   macro avg     0.5832    0.2708    0.3547       521\n",
            "weighted avg     0.7038    0.3570    0.4588       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.5500    0.1429    0.2268        77\n",
            "           1     0.7895    0.5769    0.6667        26\n",
            "           2     0.7784    0.4869    0.5991       267\n",
            "           3     0.7083    0.2048    0.3178        83\n",
            "           4     0.9000    0.2647    0.4091        34\n",
            "           5     0.5217    0.3529    0.4211        34\n",
            "\n",
            "    accuracy                         0.3724       521\n",
            "   macro avg     0.6069    0.2899    0.3772       521\n",
            "weighted avg     0.7252    0.3724    0.4786       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4286    0.1169    0.1837        77\n",
            "           1     0.7895    0.5769    0.6667        26\n",
            "           2     0.7665    0.4794    0.5899       267\n",
            "           3     0.6957    0.1928    0.3019        83\n",
            "           4     0.9091    0.2941    0.4444        34\n",
            "           5     0.5500    0.3235    0.4074        34\n",
            "\n",
            "    accuracy                         0.3628       521\n",
            "   macro avg     0.5913    0.2834    0.3706       521\n",
            "weighted avg     0.7016    0.3628    0.4664       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4545    0.1299    0.2020        77\n",
            "           1     0.7895    0.5769    0.6667        26\n",
            "           2     0.7771    0.4831    0.5958       267\n",
            "           3     0.6957    0.1928    0.3019        83\n",
            "           4     0.8889    0.2353    0.3721        34\n",
            "           5     0.5500    0.3235    0.4074        34\n",
            "\n",
            "    accuracy                         0.3628       521\n",
            "   macro avg     0.5937    0.2774    0.3637       521\n",
            "weighted avg     0.7096    0.3628    0.4674       521\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4737    0.1169    0.1875        77\n",
            "           1     0.7895    0.5769    0.6667        26\n",
            "           2     0.7647    0.4869    0.5950       267\n",
            "           3     0.6957    0.1928    0.3019        83\n",
            "           4     0.8889    0.2353    0.3721        34\n",
            "           5     0.5238    0.3235    0.4000        34\n",
            "\n",
            "    accuracy                         0.3628       521\n",
            "   macro avg     0.5909    0.2760    0.3604       521\n",
            "weighted avg     0.7043    0.3628    0.4644       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4737    0.1169    0.1875        77\n",
            "           1     0.7895    0.5769    0.6667        26\n",
            "           2     0.7711    0.4794    0.5912       267\n",
            "           3     0.7273    0.1928    0.3048        83\n",
            "           4     0.8000    0.2353    0.3636        34\n",
            "           5     0.5000    0.3235    0.3929        34\n",
            "\n",
            "    accuracy                         0.3589       521\n",
            "   macro avg     0.5802    0.2750    0.3581       521\n",
            "weighted avg     0.7053    0.3589    0.4619       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4286    0.1169    0.1837        77\n",
            "           1     0.7895    0.5769    0.6667        26\n",
            "           2     0.7647    0.4869    0.5950       267\n",
            "           3     0.6957    0.1928    0.3019        83\n",
            "           4     0.8889    0.2353    0.3721        34\n",
            "           5     0.5263    0.2941    0.3774        34\n",
            "\n",
            "    accuracy                         0.3608       521\n",
            "   macro avg     0.5848    0.2718    0.3567       521\n",
            "weighted avg     0.6978    0.3608    0.4623       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.5000    0.1299    0.2062        77\n",
            "           1     0.7895    0.5769    0.6667        26\n",
            "           2     0.7588    0.4831    0.5904       267\n",
            "           3     0.6957    0.1928    0.3019        83\n",
            "           4     0.8000    0.2353    0.3636        34\n",
            "           5     0.5238    0.3235    0.4000        34\n",
            "\n",
            "    accuracy                         0.3628       521\n",
            "   macro avg     0.5811    0.2774    0.3613       521\n",
            "weighted avg     0.6994    0.3628    0.4642       521\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.0000    0.0000    0.0000         0\n",
            "           0     0.4091    0.1169    0.1818        77\n",
            "           1     0.7895    0.5769    0.6667        26\n",
            "           2     0.7619    0.4794    0.5885       267\n",
            "           3     0.7083    0.2048    0.3178        83\n",
            "           4     0.8000    0.2353    0.3636        34\n",
            "           5     0.5789    0.3235    0.4151        34\n",
            "\n",
            "    accuracy                         0.3608       521\n",
            "   macro avg     0.5783    0.2767    0.3619       521\n",
            "weighted avg     0.6931    0.3608    0.4632       521\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51Fi13UI2N5_"
      },
      "source": [
        "for model in op_thresh_model_list:\n",
        "  test_pred = model.predict(t_p.toarray())\n",
        "  test['Target']= LabelEncoder().fit_transform(test['Target'])\n",
        "  classification_report_test = classification_report(test['Target'],test_pred,digits=4)\n",
        "  print(classification_report_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}