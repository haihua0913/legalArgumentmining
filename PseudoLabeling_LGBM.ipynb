{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PseudoLabeling_LGBM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNK93zLJMusmxlUFhHmxZGi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssvadla/Legal_Text_Classification/blob/main/PseudoLabeling_LGBM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5CnuhNj2R72",
        "outputId": "617ea309-8502-44e3-e5e4-7a4f844fce85"
      },
      "source": [
        "\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import metrics\n",
        "import pandas as pd\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import statistics\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "from matplotlib import pyplot\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn import svm\n",
        "from sklearn.utils import resample\n",
        "from sklearn.metrics import classification_report\n",
        "import math\n",
        "from nltk.corpus import stopwords\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbtyWiyO2IEC",
        "outputId": "a3acc265-c924-4b93-f32d-62a7b0c70744"
      },
      "source": [
        "train1 = pd.read_csv('/content/drive/My Drive/Research/train_data1.csv')\n",
        "train2 = pd.read_csv('/content/drive/My Drive/Research/train_data2.csv')\n",
        "train3 = pd.read_csv('/content/drive/My Drive/Research/train_data3.csv')\n",
        "train4 = pd.read_csv('/content/drive/My Drive/Research/train_data4.csv')\n",
        "train5 = pd.read_csv('/content/drive/My Drive/Research/train_data5.csv')\n",
        "train6 = pd.read_csv('/content/drive/My Drive/Research/train_data6.csv')\n",
        "train7 = pd.read_csv('/content/drive/My Drive/Research/train_data7.csv')\n",
        "train8 = pd.read_csv('/content/drive/My Drive/Research/train_data8.csv')\n",
        "train9 = pd.read_csv('/content/drive/My Drive/Research/train_data9.csv')\n",
        "train10 = pd.read_csv('/content/drive/My Drive/Research/train_data10.csv')\n",
        "train_highKappa = pd.read_csv('/content/drive/My Drive/Research/train_data_highkappa.csv')\n",
        "    #train1.head()\n",
        "train = train1\n",
        "train_list = [train2,train3,train4,train5,train6,train7,train8,train9,train10,train_highKappa]\n",
        "for i in train_list:\n",
        "  #print(i)\n",
        "  train = train.append(i)\n",
        "  \n",
        "train.sort_values(\"Sentence\", inplace = True)\n",
        "print(len(train))\n",
        "\n",
        "train = train.drop_duplicates(subset =\"Sentence\")\n",
        "\n",
        "train['Target'].unique()\n",
        "\n",
        "\n",
        "train['Target']=train['Target'].replace(['Others'],'Invalid')\n",
        "train['Target'].unique()\n",
        "\n",
        "\n",
        "\n",
        "#cleaning\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stopword=nltk.corpus.stopwords.words('english')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wl= WordNetLemmatizer()\n",
        "\n",
        "#cleaning data\n",
        "def clean_text(text):\n",
        "  text=\"\".join([word.lower() for word in text if word not in string.punctuation])\n",
        "  tokens = re.split('\\W+',text)\n",
        "  text = [wl.lemmatize(word) for word in tokens if word not in stopword]\n",
        "  return text\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vect = TfidfVectorizer(analyzer = clean_text)\n",
        "X_tfidf = tfidf_vect.fit_transform(train['Sentence'])\n",
        "\n",
        "#reading the test data\n",
        "test = pd.read_csv(r'/content/drive/My Drive/Research/test_data.csv')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37711\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "(4416, 7374)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6art2Cm85K_W",
        "outputId": "24fc1abc-adb8-4ab8-fb16-5c8d2ea3cf35"
      },
      "source": [
        "X_train, x_val, Y_train, y_val = train_test_split(X_tfidf,train['Target'],test_size=0.20,random_state=42)\n",
        "#Initialize the classifier\n",
        "classifier = lgb.LGBMClassifier()\n",
        "classifier.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
              "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
              "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
              "               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
              "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
              "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8i26LLrCeAj4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZNZPSYm2ypV",
        "outputId": "1e759240-d641-4e92-ced7-3a30bed801d7"
      },
      "source": [
        "#Unlabel data preprocessing\n",
        "unlabel = pd.read_csv(r'/content/drive/My Drive/Research/Unlabeled_data.csv')\n",
        "#unlabel.head()\n",
        "\n",
        "del unlabel['Complete']\n",
        "del unlabel['Unnamed: 0']\n",
        "\n",
        "unlabel['text'] = unlabel['text'].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\n",
        "unlabel['text'] = unlabel['text'].str.replace('[^\\w\\s]','')\n",
        "from nltk.corpus import stopwords\n",
        "words = stopwords.words('english')\n",
        "unlabel['text'] = unlabel['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in words))\n",
        "\n",
        "\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "unlabel['text'] = unlabel['text'].apply(lambda x: TextBlob(x).words)\n",
        "unlabel['text'] = unlabel['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x]))\n",
        "\n",
        "\n",
        "\n",
        "def index_reset(unlabel_2):\n",
        "  unlabel_2.reset_index(inplace=True)\n",
        "  del unlabel_2['index']\n",
        "  return unlabel_2\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slNzWMVje0OQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGkNqsLsXLKX"
      },
      "source": [
        "unlabel_size_list = [500,1000]\n",
        "#unlabel_size = 500 #comment\n",
        "Threshold = 0.9 # comment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXmDmFUR0IDZ"
      },
      "source": [
        "train = train.rename(columns={'Sentence':'text'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYe37wsF3DJ_",
        "outputId": "3bd10b9e-0081-4362-f145-811a27bd025e"
      },
      "source": [
        "\n",
        "\n",
        "record_unlabel_size_list = []\n",
        "record_concat_train_size_list = []\n",
        "record_upsample_size = []\n",
        "record_cross_f1 = [] \n",
        "record_cross_accuracy = []\n",
        "record_SS_CR = []\n",
        "record_SS_F1 = []\n",
        "\n",
        "#looping through the unlabel data\n",
        "for unlabel_size in unlabel_size_list:\n",
        "\n",
        "  record_unlabel_size_list.append(unlabel_size)\n",
        "  unlabel_1 = unlabel.loc[:unlabel_size]\n",
        "  unlabel_1 = index_reset(unlabel_1)\n",
        "  tfidf_vect = TfidfVectorizer(analyzer = clean_text)\n",
        "  X_tfidf = tfidf_vect.fit_transform(train['text'])\n",
        "  x_un1 = tfidf_vect.transform(unlabel_1['text'])\n",
        "  print(x_un1.shape)\n",
        "  pred_unlabel_1 = classifier.predict_proba(x_un1)\n",
        "  pred_unlabel_1\n",
        "  x_un1.shape\n",
        "  import numpy as np\n",
        "  pos=[]\n",
        "  large=[]\n",
        "  ind = []\n",
        "  i=0\n",
        "  for j in pred_unlabel_1:\n",
        "    if max(j)> Threshold:\n",
        "      ind.append(np.argmax(j))\n",
        "      large.append(max(j))\n",
        "      pos.append(i)\n",
        "    i+=1\n",
        "  unlabel_1 = unlabel_1.loc[pos,:]\n",
        "  train_data_size = len(unlabel_1)\n",
        "  class_x_un1 = tfidf_vect.transform(unlabel_1['text'])\n",
        "  class_x_un1.shape\n",
        "  class_pred_unlabel_1 = classifier.predict(class_x_un1)\n",
        "  class_pred_unlabel_1\n",
        "  unlabel_1['Target']=class_pred_unlabel_1\n",
        "  train = train.rename(columns={'Sentence':'text'})\n",
        "  frame_1 = [train,unlabel_1]\n",
        "  train_1 = pd.concat(frame_1)\n",
        "  record_concat_train_size_list.append(len(train_1))\n",
        "  target_values = np.unique(train_1['Target'].values)\n",
        "  data_list = []\n",
        "  data_length_list = []\n",
        "  for i in target_values:\n",
        "    df_k = train_1[train_1['Target']==i]\n",
        "    data_list.append(df_k)\n",
        "    data_length_list.append(len(df_k))\n",
        "  maximum_data = max(data_length_list)\n",
        "  ratio = math.floor(( 4 / 6 )* maximum_data)\n",
        "  loop_count = 0\n",
        "  train_upsampled = []\n",
        "  for i in data_length_list:\n",
        "    if i < ratio:\n",
        "      df_upsampled = resample(data_list[loop_count],replace=True,n_samples=ratio,random_state=123)\n",
        "    else:\n",
        "      df_upsampled = resample(data_list[loop_count],replace=True,n_samples=i,random_state=123)\n",
        "    train_upsampled.append(df_upsampled)\n",
        "    loop_count = loop_count + 1\n",
        "  df_res = pd.concat(train_upsampled)\n",
        "  record_upsample_size.append(len(df_res))\n",
        "  tfidf_vect = TfidfVectorizer(analyzer = clean_text)\n",
        "  X_tfidf = tfidf_vect.fit_transform(df_res['text'])\n",
        "  X_tfidf_df=pd.DataFrame(X_tfidf.toarray())\n",
        "  X_tfidf_df.columns=tfidf_vect.get_feature_names()\n",
        "  X_train, x_val, Y_train, y_val = train_test_split(X_tfidf,df_res['Target'],test_size=0.20,random_state=42)\n",
        "  classifier_1 = lgb.LGBMClassifier()\n",
        "  classifier_1.fit(X_train, Y_train)\n",
        "  y_pred = classifier_1.predict(x_val)\n",
        "  Accuracy_score = accuracy_score(y_val, y_pred)\n",
        "  cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
        "  scores_f1 = cross_val_score(classifier_1,x_val ,y_val, scoring = 'f1_weighted', cv=cv, n_jobs=-1).mean()\n",
        "  record_cross_f1.append(scores_f1)\n",
        "  scores_acc = cross_val_score(classifier_1,x_val ,y_val, scoring = 'accuracy', cv=cv, n_jobs=-1).mean()\n",
        "  record_cross_accuracy.append(scores_acc)\n",
        "  test['Target']=test['Target'].replace(['Others'],'Invalid')\n",
        "  test['Sentence'] = test['Sentence'].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\n",
        "  test['Sentence'] = test['Sentence'].str.replace('[^\\w\\s]','')\n",
        "  words = stopwords.words('english')\n",
        "  test['Sentence'] = test['Sentence'].apply(lambda x: \" \".join(x for x in x.split() if x not in words))\n",
        "  t_p = tfidf_vect.transform(test['Sentence'])\n",
        "  test_pred = classifier_1.predict(t_p)\n",
        "  classification_report_semi_supervised = classification_report(test['Target'],test_pred, digits=4)\n",
        "  record_SS_CR.append(classification_report_semi_supervised)\n",
        "  f1_Score_metric = f1_score(test['Target'],test_pred,average='weighted')\n",
        "  record_SS_F1.append(f1_Score_metric)\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(501, 7374)\n",
            "57\n",
            "678\n",
            "678\n",
            "@ 1526\n",
            "235\n",
            "235\n",
            "@ 1526\n",
            "2289\n",
            "2289\n",
            "@ 2289\n",
            "667\n",
            "667\n",
            "@ 1526\n",
            "269\n",
            "269\n",
            "@ 1526\n",
            "335\n",
            "335\n",
            "@ 1526\n",
            "(1001, 7374)\n",
            "139\n",
            "678\n",
            "678\n",
            "@ 1571\n",
            "240\n",
            "240\n",
            "@ 1571\n",
            "2357\n",
            "2357\n",
            "@ 2357\n",
            "674\n",
            "674\n",
            "@ 1571\n",
            "270\n",
            "270\n",
            "@ 1571\n",
            "336\n",
            "336\n",
            "@ 1571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61qMH0PWXlKp"
      },
      "source": [
        "\n",
        "#write the output to a text file\n",
        "with open(r'/content/drive/My Drive/Results/LGBM_UL_Size_Test_cross_01/Result.txt', 'a') as writefile:\n",
        "  writefile.write(\" Semi UL test experiment : \")\n",
        "  writefile.write(\"\\n\")\n",
        "  writefile.write(\" record_unlabel_size_list \")\n",
        "  writefile.write(\"\\n\")\n",
        "  writefile.write(str(record_unlabel_size_list))\n",
        "  writefile.write(\"\\n\")\n",
        "  writefile.write(\" record_concat_train_size_list \")\n",
        "  writefile.write(\"\\n\")\n",
        "  writefile.write(str(record_concat_train_size_list))\n",
        "  writefile.write(\"\\n\")\n",
        "  writefile.write(\" record_upsample_size \")\n",
        "  writefile.write(\"\\n\")\n",
        "  writefile.write(str(record_upsample_size))\n",
        "  writefile.write(\"\\n\")\n",
        "  writefile.write(\" record_cross_f1 \")\n",
        "  writefile.write(\"\\n\")\n",
        "  writefile.write(str(record_cross_f1))\n",
        "  writefile.write(\"\\n\")\n",
        "  writefile.write(\" record_cross_accuracy\")\n",
        "  writefile.write(\"\\n\")\n",
        "  writefile.write(str(record_cross_accuracy))\n",
        "  writefile.write(\"\\n\")\n",
        "  writefile.write(\" record_SS_CR \")\n",
        "  writefile.write(\"\\n\")\n",
        "  writefile.write(str(record_SS_CR))\n",
        "  writefile.write(\"\\n\")\n",
        "  writefile.write(\" record_SS_F1\")\n",
        "  writefile.write(\"\\n\")\n",
        "  writefile.write(str(record_SS_F1))\n",
        "  writefile.write(\"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}